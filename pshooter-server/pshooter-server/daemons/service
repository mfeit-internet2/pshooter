#!/usr/bin/python
#
# Execute pShooter measurements
#

import collections
import daemon
import datetime
import errno
import optparse
import pscheduler
import psycopg2
import psycopg2.extensions
import Queue
import select
import threading
import time

from resolver import HybridResolver
from taskrunner import TaskRunner
from testrunner import run_test

pscheduler.set_graceful_exit()

# Gargle the arguments

opt_parser = optparse.OptionParser()

# Daemon-related options

opt_parser.add_option("--daemon",
                      help="Daemonize",
                      action="store_true",
                      dest="daemon", default=False)
opt_parser.add_option("--pid-file",
                      help="Location of PID file",
                      action="store", type="string", dest="pidfile",
                      default=None)

# Program options

opt_parser.add_option("--dsn",
                      help="Database connection string",
                      action="store", type="string", dest="dsn",
                      default="")
opt_parser.add_option("--auth",
                      help="Colon-separated pScheduler authorization key and password",
                      action="store", type="string", dest="auth",
                      default=":")
opt_parser.add_option("--max-parallel",
                      help="Maximum concurrent tasks",
                      action="store", type="int", dest="max_parallel",
                      default=20)
opt_parser.add_option("--refresh",
                      help="Forced refresh interval (ISO8601)",
                      action="store", type="string", dest="refresh",
                      default="PT1M")

opt_parser.add_option("--maintenance-interval",
                      help="Minimum interval between internal maintenances (ISO8601)",
                      action="store", type="string", dest="maintenance_interval",
                      default="PT5M")

opt_parser.add_option("--verbose", action="store_true", dest="verbose", default=False)
opt_parser.add_option("--debug", action="store_true", dest="debug", default=False)
opt_parser.add_option("--debug-updater", action="store_true", dest="debug_updater", default=False)

(options, args) = opt_parser.parse_args()

max_parallel = options.max_parallel
if max_parallel < 1:
    opt_parser.error("Maximum parallel tasks must be at least 1.")

try:
    refresh = pscheduler.iso8601_as_timedelta(options.refresh)
except ValueError:
    opt_parser.error('Invalid refresh interval "' + options.refresh + '"')

try:
    maintenance_interval = pscheduler.iso8601_as_timedelta(options.maintenance_interval)
except ValueError:
    opt_parser.error('Invalid maintenance interval "' + options.maintenance_interval + '"')


log = pscheduler.Log(
    verbose=options.verbose,
    debug=options.debug,
    facility=pscheduler.local5
)

auth = pscheduler.string_from_file(options.auth)

try:
    auth_name, auth_password = auth.split(":")
except ValueError:
    log.error("Invalid pScheduler authorization data")
    pscheduler.fail("Invalid pScheduler authorization data")
    

dsn = pscheduler.string_from_file(options.dsn)



#
# Database updater.  This serializes database updates but is just fine
# because this is a relatively low-volume activity.
#

class DatabaseUpdater(object):

    _STATES = [
        # Pending should only be used by the database on insert
        # Prep should only be used by the main program.
        "trace",
        "running",
        "callback",
        "finished",
        "failed"
    ]


    def __init__(self, dsn, log, debug):

        self.db = None
        self.log = log
        self.debug = debug

        # A queue for getting work from other threads.  Things stay
        # here just long enough to be moved to the deque, which
        # doesn't have all of the nice thready features.
        self.queue = Queue.Queue()

        # This is the actual stack of work.  The right end is the top, the
        # left end is the bottom.
        self.deck = collections.deque()

        self.worker = threading.Thread(target=lambda: self.run())
        self.worker.setDaemon(True)
        self.worker.start()


    def __query(self, query, args):
        if self.db is None:
            self.db = pscheduler.pg_connection(dsn)
            self.debug and self.log.debug("Updater: Connected")

        self.debug and self.log.debug("Updater: Query: %s", query)
        self.debug and self.log.debug("Updater: Args : %s", str(args))

        try:
            with self.db.cursor() as cursor:
                cursor.execute(query, args)
        except Exception as ex:
            self.log.exception()
            self.debug and self.log.debug("Updater: Disconnecting database")
            self.db.close()
            self.db = None
            return False

        return True


    def run(self):

        self.log.debug("Updater: Started.")

        while True:

            # Work off everything on the deck
            while len(self.deck):
                query, args = self.deck.pop()
                if not self.__query(query, args):
                    # If it failed, put it back.
                    self.deck.append((query, args))

            # Wait for something new and add it to the bottom of the deque
            self.deck.appendleft(self.queue.get(True))



    def __call__(self,
                 task,
                 state=None,   # String in __STATES
                 path=None,    # Supplied or traced path
                 eta=None,     # Datetime
                 result=None,  # Anything that can be json_dump'd
                 diags=None    # List of strings
    ):

        sets = []
        args = []

        if state is not None:
            assert state in self._STATES
            sets.append("state = task_state_%s()" % (state))

        if path is not None:
            assert isinstance(path, list)
            sets.append("path = %s")
            args.append(pscheduler.json_dump(path))

        if eta is not None:
            assert isinstance(eta, datetime.datetime)
            sets.append("eta = %s")
            args.append(pscheduler.datetime_as_iso8601(eta))

        if result is not None:
            sets.append("result = %s")
            args.append(pscheduler.json_dump(result))

        if diags is not None:
            assert isinstance(diags, list)
            sets.append("diags = %s")
            args.append("\n".join(diags))

        assert sets, "Something called with nothing to set."

        query = "UPDATE task SET " + ", ".join(sets) + " WHERE id = %s"
        args.append(task)

        self.queue.put((query, args))




#
# Worker thread for a single task
#

class TaskWorker(object):

    def __init__(self, task, spec, hints, log, updater, auth_name, auth_password):

        self.task = task
        self.spec = spec
        self.hints = hints
        self.log = log
        self.updater = updater
        self.auth_name = auth_name
        self.auth_password = auth_password

        self.diags = []

        self.worker = threading.Thread(target=lambda: self.__run_wrapper())
        self.worker.setDaemon(True)

        self.log.debug("%d: Initialized", task)


    def start(self):
        self.worker.start()


    def _trace(self):
        self.log.debug("%d: Starting trace to %s", self.task, self.spec["path"])
        self.log.debug("%d: Requester is %s", self.hints["requester"])

        runner = TaskRunner(
            # Task
            {
                "type": "trace",
                "spec": {
                    "schema": 1,
                    "dest": self.spec["path"]
                }
            },
            # Participants
            ["localhost"],
            # A and Z ends
            { "pscheduler": "localhost", "host": "localhost" },
            { "pscheduler": "localhost", "host": self.spec["path"] },
            self.log,
            self.task,
            self.hints["requester"],
            # Authorization
            self.auth_name,
            self.auth_password
        )

        trace_result = runner.result()
        try:
            result = trace_result["results"]["application/json"]
        except KeyError:
            self.diags.append("Trace failed: %s" % ( "; ".join(trace_result["diags"])))
            return None
        self.log.debug("%d: Trace complete" % (self.task))
        self.diags.append("Trace diagnostics:\n  %s"
                          % ("\n  ".join(trace_result["diags"])))

        if not result.get("succeeded", False):
            self.diags.append("Trace failed: %s"
                              % (result.get("error", "No error message available")))
            return 

        path = [ value
                 for value in [ hop.get("ip", None) for hop in result["paths"][0]]
                 if value is not None ]
        return path



    def run(self):

        # 
        # Use the supplied path or trace one
        #

        if isinstance(self.spec["path"], list):
            path = self.spec["path"]
        else:
            self.updater(self.task, state="trace")
            path = self._trace()
            if path is None:
                self.log.debug("%d: Trace failed", self.task)
                self.end_state = "failed"
                self.updater(self.task, self.end_state, diags=self.diags)
                return


        #
        # Prepare the path for pShooter
        #

        # Scrub out any local interfaces
        local_ips = pscheduler.LocalIPList()
        path = [ hop
                 for hop in path
                 if hop not in local_ips ]

        # Figure out what local interface will be used to reach the
        # first hop and prepend that to the path.

        local_interface = pscheduler.source_interface(path[0])[0] 
        self.log.debug("%d: Prepending local outbound interface %s",
                       self.task, local_interface)
        path.insert(0, local_interface)
        self.log.debug("%d: Path is %s" % (self.task, path))

        self.path = path
        self.updater(self.task, path=path)


        #
        # Run the test
        #

        self.log.debug("%d: pShooting", self.task)
        self.updater(self.task, state="running")

        resolver = HybridResolver(self.spec.get("dns", {}))
        result = run_test(
            {
                "path": self.path,
                "test": self.spec["test"]
            },
            resolver,
            self.log,
            self.task,
            self.hints["requester"],
            auth_name,
            auth_password
        )

        self.diags.append("Finished")

        self.updater(self.task,
                     result=result,
                     diags=self.diags
                     )
        self.end_state = "finished"


    def __callback(self):

        if "callback" not in self.spec:
            return

        self.log.debug("%d: Starting callback", self.task)
        self.updater(self.task, state="callback")

        # The callback can be assumed to have been validated on the way in.

        cb_spec = self.spec["callback"]

        retry_policy = cb_spec.get("retry-policy",
                                   {"attempts": 1, "wait": "PT1S"})
        retry = pscheduler.RetryPolicy(retry_policy)

        attempt = 0
        while True:
            self.log.debug("%d: Callback attempt %d", self.task, attempt)
            (status, text) = pscheduler.url_get(cb_spec["_href"],
                                                bind=cb_spec.get("_bind", None),
                                                params=cb_spec.get("_params", None),
                                                headers=cb_spec.get("_headers", None),
                                                throw=False,
                                                json=False)
            self.log.debug("%d: Callback returned %d: %s", self.task, status, text)

            if status == 200:
                self.log.debug("Callback succeeded.")
                self.diags.append("Callback succeeded.")
                break

            self.diags.append("Callback attempt failed: %d: %s" % (status, text))

            retry_sleep = retry.retry(attempt)

            if retry_sleep is None:
                message = "Ran out of callback retries.  Giving up."
                self.log.debug("%d: %s", self.task, message)
                self.diags.append(message)
                break

            self.log.debug("%d: Sleeping %s until next retry", self.task, retry_sleep)
            time.sleep(pscheduler.timedelta_as_seconds(retry_sleep))

            attempt += 1


    def __run_wrapper(self):

        self.log.debug("%s: Running" % (self.task))

        try:
            self.run()  # This will set end_state
        except Exception as ex:
            self.log.exception()
            self.diags.append("Exception while running: %s" % (ex))
            self.end_state = "failed"

        self.__callback()
        self.updater(self.task, state=self.end_state, diags=self.diags)
        self.log.debug("%s: Complete", self.task)
                
        




#
# Main Program
#


def main_program():

    log.debug("Begin main")

    db = pscheduler.pg_connection(dsn)
    log.debug("Connected to DB")

    # Listen for notifications.

    for listen in ["task_new", "task_finished"]:
        log.debug("Listening for notification %s" % (listen))
        with db.cursor() as cursor:
            cursor.execute("LISTEN %s" % (listen))

    updater = DatabaseUpdater(dsn, log, options.debug_updater)


    # Force anything that looks like it was running into a failed state.
    with db.cursor() as cursor:
        cursor.execute("DO $$ BEGIN PERFORM task_reset(); END $$")


    # Prime this for the first run
    wait_time = datetime.timedelta()

    # Note that this will not work prior to 1970.  Please drop me a
    # line if time travel is invented and this becomes aan issue.  :-)
    last_maintenance = pscheduler.time_epoch()


    while True:

        # Do housekeeping if it's that time.
        if (pscheduler.time_now() - last_maintenance) > maintenance_interval:
            log.debug("Doing maintenance")
            with db.cursor() as maint:
                maint.execute("DO $$ BEGIN PERFORM maintain(); END $$")
                last_maintenance = pscheduler.time_now()

        log.debug("Next check in %s", wait_time)
        if not pscheduler.timedelta_is_zero(wait_time):

            # Wait for a notification or the wait time to elapse.  Eat all
            # notifications as a group; we only care that we were notified.

            try:
                if pscheduler.polled_select(
                        [db],[],[],
                        pscheduler.timedelta_as_seconds(wait_time)) \
                    != ([],[],[]):
                    # Notified
                    db.poll()
                    del db.notifies[:]
                    log.debug("Notified.")

            except select.error as ex:

                err_no, message = ex
                if err_no != errno.EINTR:
                    log.exception()
                    raise ex


        with db.cursor() as cursor:

            # Get all pending tasks up to the number we're allowed to
            # run in parallel.

            cursor.execute("""
                       SELECT id, spec, hints, added FROM (
                           SELECT
                               task.id,
                               task.spec,
                               task.hints,
                               task.state,
                               task_state.running AS pick_order,
                               task.added
                           FROM
                               task
                               JOIN task_state ON task_state.id = task.state
                           WHERE
                               NOT task_state.finished
                           ORDER BY
                               pick_order DESC,  -- Put runners first
                               added ASC
                           LIMIT %s
                       ) t
                       WHERE state = task_state_pending()
                       ORDER BY added -- Get tasks in FIFO order
                   """, [max_parallel])


            wait_time = refresh

            workers = []
            ids = []

            for row in cursor:

                task, spec, hints, added = row
                log.debug("Starting %d, added %s", task, added)
                worker = TaskWorker(task, spec, hints, log, updater, auth_name, auth_password)
                workers.append(worker)
                ids.append(task)

            # This serves as an immediate interlock for all tasks that
            # were started so they're not picked up by the next
            # iteration of this loop.

            if ids:
                with db.cursor() as update_cursor:
                    update_cursor.execute(
                        "UPDATE task SET state = task_state_prep() WHERE id IN %s",
                        [tuple(ids)])

                wait_time = datetime.timedelta()

            # Let the workers do their things
            for worker in workers:
                worker.start()


    # Not that this will ever be reached...
    db.close()



if options.daemon:
    pidfile = pscheduler.PidFile(options.pidfile)
    with daemon.DaemonContext(pidfile=pidfile):
        pscheduler.safe_run(lambda: main_program())
else:
    pscheduler.safe_run(lambda: main_program())
